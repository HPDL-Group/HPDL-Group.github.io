<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Tong Yang">

<title>成果展示三</title>

<link rel="icon" href="/favicon/favicon-32x32.png" type="image/x-icon">
<link rel="stylesheet" href="/main.min.css" media="screen">
<link rel="stylesheet" href="/custom.css" media="screen">

<!--Made with Geekdoc theme https://github.com/xoxys/hugo-geekdoc-->

</head>

<body>
    <svg style="position: absolute; width: 0; height: 0; overflow: hidden;" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><symbol viewBox="0 0 24 24" id="arrow_back" xmlns="http://www.w3.org/2000/svg"><path d="M20.016 11.016v1.969H7.828l5.578 5.625L12 20.016 3.984 12 12 3.984l1.406 1.406-5.578 5.625h12.188z"/></symbol><symbol viewBox="0 0 24 24" id="arrow_left_alt" xmlns="http://www.w3.org/2000/svg"><path d="M7.969 11.016v-3L3.985 12l3.984 3.984v-3h12.047v-1.969H7.969z"/></symbol><symbol viewBox="0 0 24 24" id="arrow_right_alt" xmlns="http://www.w3.org/2000/svg"><path d="M16.031 11.016v-3L20.015 12l-3.984 3.984v-3H3.984v-1.969h12.047z"/></symbol><symbol viewBox="0 0 24 24" id="bookmark" xmlns="http://www.w3.org/2000/svg"><path d="M15 5.016q.797 0 1.406.586t.609 1.383v16.031l-7.031-3-6.984 3V6.985q0-.797.609-1.383t1.406-.586h9.984zM18.984 18V5.016q0-.797-.586-1.406t-1.383-.609H6.984q0-.797.609-1.406T8.999.986h9.984q.797 0 1.406.609t.609 1.406v15.984z"/></symbol><symbol viewBox="0 0 16 28" id="code" xmlns="http://www.w3.org/2000/svg"><path d="M4.5 23a1.5 1.5 0 10-3.001.001A1.5 1.5 0 004.5 23zm0-18a1.5 1.5 0 10-3.001.001A1.5 1.5 0 004.5 5zm10 2a1.5 1.5 0 10-3.001.001A1.5 1.5 0 0014.5 7zM16 7a3.002 3.002 0 01-1.5 2.594c-.047 5.641-4.047 6.891-6.703 7.734C5.313 18.109 4.5 18.484 4.5 20v.406A3 3 0 016 23a3.001 3.001 0 01-6 0c0-1.109.609-2.078 1.5-2.594V7.594A3 3 0 010 5a3.001 3.001 0 016 0 3.002 3.002 0 01-1.5 2.594v7.766c.797-.391 1.641-.656 2.406-.891 2.906-.922 4.562-1.609 4.594-4.875A3 3 0 0110 7a3.001 3.001 0 016 0z"/></symbol><symbol viewBox="0 0 24 24" id="date" xmlns="http://www.w3.org/2000/svg"><path d="M18.984 20.016V9H5.015v11.016h13.969zm0-16.032q.797 0 1.406.609t.609 1.406v14.016q0 .797-.609 1.383t-1.406.586H5.015q-.844 0-1.43-.563t-.586-1.406V5.999q0-.797.586-1.406t1.43-.609h.984V2.015h2.016v1.969h7.969V2.015H18v1.969h.984zm-1.968 7.032v1.969H15v-1.969h2.016zm-4.032 0v1.969h-1.969v-1.969h1.969zm-3.984 0v1.969H6.984v-1.969H9z"/></symbol><symbol viewBox="0 0 24 24" id="download" xmlns="http://www.w3.org/2000/svg"><path d="M5.016 18h13.969v2.016H5.016V18zm13.968-9L12 15.984 5.016 9H9V3h6v6h3.984z"/></symbol><symbol viewBox="0 0 24 24" id="email" xmlns="http://www.w3.org/2000/svg"><path d="M20.016 8.016V6L12 11.016 3.984 6v2.016L12 12.985zm0-4.032q.797 0 1.383.609t.586 1.406v12q0 .797-.586 1.406t-1.383.609H3.985q-.797 0-1.383-.609t-.586-1.406v-12q0-.797.586-1.406t1.383-.609h16.031z"/></symbol><symbol viewBox="0 0 24 28" id="github" xmlns="http://www.w3.org/2000/svg"><path d="M12 2c6.625 0 12 5.375 12 12 0 5.297-3.437 9.797-8.203 11.391-.609.109-.828-.266-.828-.578 0-.391.016-1.687.016-3.297 0-1.125-.375-1.844-.812-2.219 2.672-.297 5.484-1.313 5.484-5.922 0-1.313-.469-2.375-1.234-3.219.125-.313.531-1.531-.125-3.187-1-.313-3.297 1.234-3.297 1.234a11.28 11.28 0 00-6 0S6.704 6.656 5.704 6.969c-.656 1.656-.25 2.875-.125 3.187-.766.844-1.234 1.906-1.234 3.219 0 4.594 2.797 5.625 5.469 5.922-.344.313-.656.844-.766 1.609-.688.313-2.438.844-3.484-1-.656-1.141-1.844-1.234-1.844-1.234-1.172-.016-.078.734-.078.734.781.359 1.328 1.75 1.328 1.75.703 2.141 4.047 1.422 4.047 1.422 0 1 .016 1.937.016 2.234 0 .313-.219.688-.828.578C3.439 23.796.002 19.296.002 13.999c0-6.625 5.375-12 12-12zM4.547 19.234c.031-.063-.016-.141-.109-.187-.094-.031-.172-.016-.203.031-.031.063.016.141.109.187.078.047.172.031.203-.031zm.484.532c.063-.047.047-.156-.031-.25-.078-.078-.187-.109-.25-.047-.063.047-.047.156.031.25.078.078.187.109.25.047zm.469.703c.078-.063.078-.187 0-.297-.063-.109-.187-.156-.266-.094-.078.047-.078.172 0 .281s.203.156.266.109zm.656.656c.063-.063.031-.203-.063-.297-.109-.109-.25-.125-.313-.047-.078.063-.047.203.063.297.109.109.25.125.313.047zm.891.391c.031-.094-.063-.203-.203-.25-.125-.031-.266.016-.297.109s.063.203.203.234c.125.047.266 0 .297-.094zm.984.078c0-.109-.125-.187-.266-.172-.141 0-.25.078-.25.172 0 .109.109.187.266.172.141 0 .25-.078.25-.172zm.906-.156c-.016-.094-.141-.156-.281-.141-.141.031-.234.125-.219.234.016.094.141.156.281.125s.234-.125.219-.219z"/></symbol><symbol viewBox="0 0 28 28" id="heart" xmlns="http://www.w3.org/2000/svg"><path d="M14 26c-.25 0-.5-.094-.688-.281l-9.75-9.406c-.125-.109-3.563-3.25-3.563-7C-.001 4.735 2.796 2 7.468 2c2.734 0 5.297 2.156 6.531 3.375C15.233 4.156 17.796 2 20.53 2c4.672 0 7.469 2.734 7.469 7.313 0 3.75-3.437 6.891-3.578 7.031l-9.734 9.375a.972.972 0 01-.688.281z"/></symbol><symbol viewBox="0 0 24 24" id="link" xmlns="http://www.w3.org/2000/svg"><path d="M17.016 6.984q2.063 0 3.516 1.477T21.985 12t-1.453 3.539-3.516 1.477h-4.031v-1.922h4.031q1.266 0 2.18-.914T20.11 12t-.914-2.18-2.18-.914h-4.031V6.984h4.031zm-9 6v-1.969h7.969v1.969H8.016zM3.891 12q0 1.266.914 2.18t2.18.914h4.031v1.922H6.985q-2.063 0-3.516-1.477T2.016 12t1.453-3.539 3.516-1.477h4.031v1.922H6.985q-1.266 0-2.18.914T3.891 12z"/></symbol><symbol viewBox="0 0 24 24" id="menu" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2.016H3V6zm0 6.984v-1.969h18v1.969H3zM3 18v-2.016h18V18H3z"/></symbol><symbol viewBox="0 0 24 24" id="notification" xmlns="http://www.w3.org/2000/svg"><path d="M18 15.984L20.016 18v.984H3.985V18l2.016-2.016v-4.969q0-2.344 1.195-4.078t3.305-2.25v-.703q0-.609.422-1.055t1.078-.445 1.078.445.422 1.055v.703q2.109.516 3.305 2.25t1.195 4.078v4.969zm-6 6q-.844 0-1.43-.563t-.586-1.406h4.031q0 .797-.609 1.383T12 21.984z"/></symbol><symbol viewBox="0 0 24 24" id="path" xmlns="http://www.w3.org/2000/svg"><path d="M21.984 11.016H15v-3h-2.016v7.969H15v-3h6.984v8.016H15v-3h-3.984V8.017H9v3H2.016V3.001H9v3h6v-3h6.984v8.016z"/></symbol><symbol viewBox="0 0 24 24" id="person" xmlns="http://www.w3.org/2000/svg"><path d="M12 14.016q2.531 0 5.273 1.102t2.742 2.883v2.016H3.984v-2.016q0-1.781 2.742-2.883t5.273-1.102zM12 12q-1.641 0-2.813-1.172T8.015 8.015t1.172-2.836T12 3.984t2.813 1.195 1.172 2.836-1.172 2.813T12 12z"/></symbol><symbol viewBox="0 0 24 24" id="search" xmlns="http://www.w3.org/2000/svg"><path d="M9.516 14.016q1.875 0 3.188-1.313t1.313-3.188-1.313-3.188-3.188-1.313-3.188 1.313-1.313 3.188 1.313 3.188 3.188 1.313zm6 0l4.969 4.969-1.5 1.5-4.969-4.969v-.797l-.281-.281q-1.781 1.547-4.219 1.547-2.719 0-4.617-1.875T3.001 9.516t1.898-4.617 4.617-1.898 4.594 1.898 1.875 4.617q0 .984-.469 2.227t-1.078 1.992l.281.281h.797z"/></symbol><symbol viewBox="0 0 20 28" id="shield" xmlns="http://www.w3.org/2000/svg"><path d="M17 15V5h-7v17.766c.797-.422 2.078-1.156 3.328-2.141C15 19.312 17 17.266 17 15zm3-12v12c0 6.578-9.203 10.734-9.594 10.906-.125.063-.266.094-.406.094s-.281-.031-.406-.094C9.203 25.734 0 21.578 0 15V3c0-.547.453-1 1-1h18c.547 0 1 .453 1 1z"/></symbol><symbol viewBox="0 0 32 32" id="tags" xmlns="http://www.w3.org/2000/svg"><path d="M5 5c-1.104 0-2 .887-2 2v8l13.381 13.381c.716.716 1.838.78 2.62.191L5 14.5V5.007 5zm11-1l13.381 13.381c.783.783.787 2.051.008 2.831l-7.177 7.177a2.003 2.003 0 01-2.831-.008L6 14V6c0-1.112.895-2 2-2h8zm-4.5 7a1.5 1.5 0 10-.001-3.001A1.5 1.5 0 0011.5 11z"/></symbol><symbol viewBox="0 0 35 32" id="telescope" xmlns="http://www.w3.org/2000/svg"><path d="M27.464 2.314a.501.501 0 00-.698-.257L14.86 8.339a.499.499 0 00-.233.621l.245.641-6.873 3.769a.5.5 0 00-.222.63l.228.549-7.299 3.488a.5.5 0 00-.246.643l1.498 3.61a.5.5 0 00.629.28l7.625-2.701.228.549a.5.5 0 00.601.289l7.276-2.097.218.569a.497.497 0 00.612.299l13-4a.498.498 0 00.317-.663l-5-12.501zM2.7 21.469l-1.134-2.734 6.823-3.261 1.439 3.47L2.7 21.469zm8.491-1.846l-.238-.574-1.843-4.445-.238-.573 6.336-3.475 2.374 6.134.375.981-6.766 1.952zm8.109-1.238l-.203-.531c-.003-.011-.001-.024-.006-.035l-.618-1.597-2.754-7.206 11.023-5.815 4.592 11.48L19.3 18.385zM28.964.314a.5.5 0 00-.929.371l6 15a.502.502 0 00.651.279.501.501 0 00.279-.65l-6.001-15zM18 21h-3c-1.14 0-2 .86-2 2v1.315l-5.879 6.859a.5.5 0 10.758.651L13.73 25H16v6.5a.5.5 0 001 0V25h2.27l5.85 6.825a.497.497 0 00.705.054.5.5 0 00.054-.705L20 24.315v-1.24C20 21.912 19.122 21 18 21zm1 3h-5v-1c0-.589.411-1 1-1h3c.57 0 1 .462 1 1.075V24z"/></symbol></svg>

    <div class="wrapper">
        <input type="checkbox" class="hidden" id="menu-control" />
        <header class="gdoc-header">
    <div class="container flex align-center justify-between">
        
        <!-- <label for="menu-control" class="gdoc-nav__control">
            <svg class="icon menu"><use xlink:href="#menu"></use></svg>
            <svg class="icon arrow-back"><use xlink:href="#arrow_back"></use></svg>
        </label> -->
        
        <a class="gdoc-header__link" href="https://HPDL-group.github.io/">
            <span class="gdoc-brand flex align-center">
                <!-- <img class="gdoc-brand__img" src="/111.png" alt="Homepage" width=60 height=60> -->
                <!-- <p>李东升团队国家杰出青年科学基金项目结题成果科普性介绍</p> -->
                <p>结题成果科普性介绍</p>
            </span>
        </a>
</header>


        <main class="container flex flex-even">
            <aside class="gdoc-nav">
                <nav>

                </nav>
            </aside>

            <div class="gdoc-page">
                
    


    



                <div class="gdoc-page__header flex flex-wrap justify-between hidden-mobile breadcrumb-container" itemscope itemtype="http://data-vocabulary.org/Breadcrumb">
                    <span>
                        <span class="breadcrumb">
                            <!-- <svg class="icon path"><use xlink:href="#path"></use></svg> -->
                            <a href='/' class="breadcrumb-link button">首页</a>
                            <a href='/news1.html' class="breadcrumb-link button">展示成果一</a>
                            <a href='/news2.html' class="breadcrumb-link button">展示成果二</a>
                            <a href='/news3.html' class="breadcrumb-link button">展示成果三</a>
                            <a href='/news4.html' class="breadcrumb-link button">展示成果四</a>
                            <a href='/news5.html' class="breadcrumb-link button">展示成果五</a>
                        </span>
                    </span>
                    <span></span>
                </div>
                <article class="gdoc-markdown">
                
                    <div class="gdoc-page__anchorwrap"><h2 id="research"><strong>（三）面向智能模型的并行训练优化技术</strong></h2></div>
                    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了提高机器学习训练的可扩展性，项目针对大规模智能模型的参数交换和数据依赖等特点展开分析研究，突破模型参数通信优化、高效并行策略构建和异构资源协同优化等关键技术。</p>
                    <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在通信优化方面，针对模型参数通信在大规模训练中面临的带宽瓶颈与同步延迟问题，项目探索了拓扑感知和稀疏感知的通信范式，并结合流水线调度实现通信隐藏，提高了模型参数的通信性能；在并行策略方面，为应对模型规模激增带来的内存与计算压力，项目突破了单一并行模式的限制，发展出适配不同模型结构与硬件资源的高效并行训练策略，提升了并行训练的效率；在异构协同方面，面向当前智能计算系统普遍存在的多类型加速器共存现状，项目建立了跨设备的任务映射、通信协调与负载均衡方法，有效提升了异构智能计算环境中的训练性能。</p>
                    
                    
                    
                    
                    <details open="" style="margin-bottom: 10px;">
                        <summary><b>a．大规模分布式训练的通信优化技术</b></summary>
                        
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在智能模型大规模分布式训练中，通信开销已成为制约系统可扩展性的核心瓶颈。随着智能模型训练规模持续增长，数据并行方法频繁的全局梯度同步而产生严重的通信瓶颈，成为制约训练效率的关键因素。此外，在混合并行训练策略则引入了复杂的通信依赖关系，包括数据并行和张量并行的AllReduce、流水线并行的点对点传输以及稀疏数据的通信等，使得先计算后通信或简单调度策略难以有效隐藏通信延迟。与此同时，智能计算集群普遍采用多层次互连网络拓扑，但现有系统往往仅利用单一链路进行通信，导致高带宽网络链路未被充分利用，低带宽链路却成为性能瓶颈。</p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;围绕上述问题，项目系统性地开展了数据并行通信优化、拓扑感知通信调度以及混合并行通信架构等研究，形成了覆盖从通信原语操作到系统框架的完整技术链条。相关研究发表于TPDS、ICPP等CCF推荐的并行与分布计算领域A类、B类国际期刊与会议。以下四项代表性工作分别从不同维度切入，共同构成了面向大规模分布式训练的高效通信优化技术体系。</p>
                        
                        <h4><i>i．</i>一种数据并行的多链路并发通信优化技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据并行是大规模分布式训练中最基础且广泛应用的并行模式，其核心在于通过全局梯度同步实现模型参数的一致性更新。进一步地，分片数据并行技术（如微软提出的ZeRO技术、Meta提出的FSDP技术）则通过将模型状态分片存储于不同设备，有效缓解了单设备的内存压力，已成为大规模数据并行训练的常用技术。然而，其依赖的AllGather/ReduceScatter等集合通信原语在大规模集群中会引发严重通信瓶颈。现有方法通常利用单一高速链路（如NVLink）进行通信，而忽略集群中同时存在的多层级互连（如PCIe、InfiniBand），导致网络链路资源闲置，无法充分发挥硬件潜力。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/HSDP.png" alt="细粒度参数分片与多链路并发的通信优化技术HSDP" style="max-width: 50%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图1：细粒度参数分片与多链路并发的通信优化技术HSDP</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种细粒度参数分片与多链路并发的通信优化技术HSDP，其核心创新在于突破传统分片数据并行技术中所有参数采用统一分片策略的局限，支持不同参数依据所在设备间的可用链路独立分片。HSDP针对各类链路分别构建独立通信组，使不同参数的通信任务可在不同链路上并行执行；同时通过设计通信原语的可交换性，例如允许ReduceScatter与AllReduce的执行顺序灵活调整，进一步提升通信操作执行的灵活性。此外，HSDP引入自动化规划器，通过构建涵盖多链路带宽、启动开销与并发冲突的成本模型，将最优分片策略的搜索转化为混合整数线性规划问题，从而自动求解最佳参数分配方案。在LLaMA、GPT等主流智能模型训练任务上的实验结果显示，相较于ZeRO、FSDP和MiCS等方法，HSDP可实现最高1.30倍的端到端训练加速，且在50Gbps低带宽InfiniBand环境下优势更为突出。该研究系统性挖掘了分片数据并行场景下多链路并发通信的潜力，为去中心化、拓扑感知的通信优化提供了新范式。</p>
                        
                        <h4><i>ii．</i>一种混合并行的通信内存联合优化技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着智能模型的层数逐渐加深，数据并行和流水线并行相结合的混合并行训练已成为大规模智能模型并行训练的重要方式。混合训练中通信模式较为复杂，其既包含AllReduce梯度同步，也包含激活值和梯度点对点传输，还可能涉及稀疏数据的通信。通信调度技术通过重叠通信与计算操作，提高并行训练的性能。然而，针对混合并行训练的通信调度技术面临通信操作种类繁多、数据依赖复杂及拓扑利用率低等挑战，导致通信无法有效隐藏，成为性能瓶颈。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/TriRace.png" alt="拓扑感知的多维通信调度技术TriRace" style="max-width: 50%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图2：拓扑感知的多维通信调度技术TriRace</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;面向复杂网络拓扑与混合并行场景下的多维通信调度难题，项目研究提出了拓扑感知的多维通信调度技术TriRace，如图 2所示，其核心为三维通信调度技术和拓扑感知的通信操作执行方法。面向混合并行训练，TriRace提出的三维通信调度从三个正交维度协同调度通信操作：AllReduce调度利用并行训练中固有的权重更新延迟窗口，将数据并行的梯度通信重叠到后续多个迭代的前向计算中；稀疏数据调度基于下一迭代的输入预取信息区分必须提前通信与可延迟通信的稀疏梯度子集，减少训练关键路径上的通信量；点对点通信调度则将双向点对点通信解耦为两个单向操作，优先保障关键路径上的单向通信，其余通信异步执行以避免阻塞。此外，TriRace拓扑感知的通信操作执行方法通过离线分析通信操作的网络链路使用情况，为不同类型通信分配全局优先级，并在运行时进行动态调度以网络避免链路竞争，从而提高了智能训练通信操作的效率。在BERT、GPT等智能模型训练任务上，TriRace相比Pipedream-2BW和Megatron-LM等方法可以实现1.07至1.45倍的训练吞吐量提升。该研究的拓扑感知调度思想有效解决了多种类通信原语的协同优化难题，显著提升了去中心化架构下通信与计算操作的重叠效率，为大规模训练提供了关键通信加速能力。</p>


                        <h4><i>iii．</i>一种拓扑感知的多维通信调度方法</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在大规模并行训练中，混合并行策略虽能有效支撑大规模参数智能模型训练的扩展，但仍面临挑战。一方面，若模型切分不合理，容易导致流水线执行过程中各设备间内存负载严重不均；另一方面，梯度同步通信往往集中在流水线迭代边界处突发进行，引发设备等待，降低整体计算效率。现有方法通常孤立地处理通信或内存问题，缺乏对二者协同优化的统一目标，难以在有限硬件资源下兼顾高吞吐与低内存占用。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/HIPPLE.png" alt="混合并行训练的通信内存联合优化技术HIPPIE" style="max-width: 38%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图3：混合并行训练的通信内存联合优化技术HIPPIE</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了混合并行训练的通信内存联合优化技术HIPPIE。HIPPIE以内存效率为核心优化目标，将内存效率定义为吞吐量与扩展性乘积除以内存开销，从而为模型切分与调度提供统一的优化准则。HIPPIE基于层粒度对计算、内存和通信开销进行精细建模，自动搜索能够最大化内存效率的模型切分方案，并智能选择激活值检查点的位置，在节省内存的同时尽量减少额外计算开销。同时HIPPIE通过将梯度AllReduce通信操作提前至前向或反向计算过程中的空闲时段，巧妙地将其隐藏于流水线气泡之中，有效消除了通信等待时间。HIPPIE方法灵活兼容混合并行策略，便于在实际智能计算系统中高效横向扩展。实验结果表明，HIPPIE能实现超过90%的扩展效率，相较DAPPLE方法最高可提升并行训练吞吐量达80%，同时降低内存开销57%，内存效率提升高达4.18倍。该研究将内存效率确立为混合并行训练的首要优化目标，其自动化规划机制与通信隐藏策略为大规模训练提供了兼具高吞吐与低内存占用的实用解决方案。</p>



                        <h4><i>iv．</i>一种张量并行的通信数据依赖优化技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在大规模智能模型训练中，张量并行通过将模型内部权重（例如矩阵乘法的行列维度）进行切分，有效降低了单个设备的内存占用，从而支持更大规模的模型部署。然而，这种切分方式引入了大量AllReduce通信操作，且这些通信与计算之间存在严格的数据依赖关系，例如，在前向或反向传播过程中，必须等待通信完成才能继续后续计算。在缺乏NVLink等高速互连的智能计算设备（如 RTX 3090 GPU）上，通信开销可占迭代时间的60%以上。现有张量并行优化方法尝试在单个算子内部内重叠通信与计算，未能充分挖掘跨越前向、重计算和反向等多个传播阶段的调度潜力。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/Oases.png" alt="细粒度的跨传播阶段通信重叠技术Oases" style="max-width: 45%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图4：细粒度的跨传播阶段通信重叠技术Oases</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种细粒度的跨传播阶段通信重叠技术Oases，其核心是结合训练操作的流水线调度以实现通信隐藏，并结合自动化并行策略搜索系统性地优化张量并行训练性能。Oases通过将输入批次划分为多个子批次，使一个子批次的通信操作能够与另一个子批次的计算任务并行执行，打破传统训练流程中各阶段之间的严格顺序约束，从而实现更高效的资源利用。同时，Oases利用重计算阶段梯度传播的数学特性，即输入梯度与输出梯度在特定条件下等价，跳过重计算中冗余的通信操作，进一步减少不必要的同步开销。在此基础上，Oases还设计了一个精准的代价模型，用于评估不同调度方案下的通信重叠效果与内存消耗，并将张量并行的策略选择形式化为整数线性规划问题，自动求解出在满足内存约束前提下通信重叠最优的配置方案。实验表明，在多种主流GPU集群上，Oases相比Megatron-LM最高可实现1.95倍的并行训练吞吐量提高和2.18倍的设备利用率提升。该研究显著缓解了张量并行的通信瓶颈，其提出的训练操作流水线调度方法、跨传播重叠调度技术与自动化策略搜索机制，为张量并行在实际场景中的高效部署提供了重要的支撑。</p>



                    </details>



                    
                    <details open="" style="margin-bottom: 10px;">
                        <summary><b>b．大规模智能模型的高效并行训练策略</b></summary>
                        
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大规模智能模型训练中单一并行策略已难以兼顾日益增长的模型规模、复杂的模型结构与多样化硬件资源配置之间的复杂权衡。随着模型参数规模的高速增长，训练过程不仅面临单设备显存不足的瓶颈，还需在不同规模的智能计算集群上高效调度计算、内存与通信资源。此外，新型模型架构引入了多种模块结构与动态参数状态，如混合专家模型因路由机制导致各设备负载动态不均，以及多模态模型由编码模块、投影模块与主干模型等结构差异显著的模块组成，并行训练的优化方法难以有效适配。同时，大规模智能模型的并行训练普遍依赖人工设计的固定并行策略，缺乏对模型结构、集群规模与运行时状态的联合感知能力，既限制了资源利用率，又增加了开发与调优成本。</p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;围绕上述问题，项目系统性地开展了多维并行训练的自动化协同优化、细粒度的负载均衡优化以及自适应的并行训练策略生成等研究，形成了一套通用、高效且可扩展的大规模智能模型并行训练技术体系。相关研究发表于TPDS、TC、TACO等CCF推荐的计算机体系结构与并行与分布计算领域A类国际期刊。以下五项代表性工作从不同维度展开研究，共同构成了覆盖多种模型结构及不同硬件资源的高效并行训练技术体系，全面支撑了大规模智能模型的高效并行训练。</p>
                        
                        <h4><i>i．</i>一种多维并行的自动化生成与优化技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着智能模型的参数量和训练数据量的爆炸式增长，GPT-3等大规模智能模型训练的内存使用量、计算量和通信量巨大，综合采用数据并行、流水线并行以及张量并行等方法的多维度并行训练方法成为大规模智能模型的主要手段。然而，现有Megatron-LM等方法针对特定模型进行定制化设计，需要并行训练专家和大模型算法设计专家共同参与训练代码的构建和优化，多维并行训练面临可编程性差、并行效率低的问题。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/Merak.png" alt="多维并行训练的自动化集成优化技术Merak" style="max-width: 35%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图5：多维并行训练的自动化集成优化技术Merak</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种面向大规模智能模型的多维并行训练自动化集成优化技术Merak，如图 2所示，其核心是设计了多维并行智能训练框架，通过采用基于符号算子的自动多维划分、面向多维并行训练的参数通信优化等方法，提高了模型多维度并行训练的可编程性和并行效率，实现了数据、流水线与张量并行策略自动化集成优化。为提高大模型训练的可编程性，设计了一种基于符号算子的计算图切分方法，在模型划分前将大模型算子替换成符号算子，在保留大模型计算图结构信息条件下降低内存开销。为提高大模型训练的并行效率，设计了一种基于次级流水化的张量并行训练方法，在张量并行内的数据维度进行划分，将数据进一步分为独立的子数据，子数据间的计算和通信操作没有数据依赖关系，然后将子数据的操作进行流水化，从而实现通信和计算操作的互相重叠，提高了大规模并行训练的计算效率。在GPT等大规模智能模型上的对比实验结果表明，相比国际主流技术英伟达Megatron-LM，Merak方法实现了1.46-1.57倍的并行训练性能加速。该研究为通用大模型的高效、便捷训练提供了一套完整的自动化解决方案。</p>
                        
                        <h4><i>ii．</i>一种面向混合专家模型架构的并行训练优化技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;混合专家（Mixture of Experts，MoE）模型在自然语言处理等领域得到了很多应用。但在MoE模型的并行训练过程中，设备负载存在显著的动态不均衡现象，严重影响了MoE模型训练的吞吐量。主流系统级负载均衡方法利用资源分配操作来动态均衡负载，在不影响模型收敛的前提下较高地提升了训练速度，但仍面临因粗粒度视角及算子间紧密数据依赖所导致的通信冗余、资源分配开销大等问题。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/Prophet.png" alt="面向混合专家模型并行训练的负载均衡优化技术Prophet" style="max-width: 55%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图6：面向混合专家模型并行训练的负载均衡优化技术Prophet</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种面向混合专家模型并行训练的负载均衡优化技术Prophet。Prophet首先定义一系列细粒度专家放置策略，为通信高效的负载均衡奠定基础。其次，Prophet引入一个性能模型，用以评测不同专家放置策略的优劣。最后，Prophet利用高效搜索算法快速找出负载较为均衡的专家放置策略，以减少负载均衡过程中存在的冗余通信。进一步地，Prophet利用样本分布的局部性预估专家负载，松弛了部分算子间数据依赖。在此基础上，Prophet将资源分配操作调度至智能计算设备的通信计算空闲处，以充分利用设备闲置时间减少资源分配开销。实验结果表明，与当前两个主流的MoE模型并行训练框架Deepspeed-MoE和FasterMoE相比，Prophet取得了1.05倍到3.22倍的并行训练性能提升。此外，相比系统级负载均衡方法FasterMoE，Prophet的负载均衡度最高可提升12.06倍。该研究在系统层面实现了对MoE动态稀疏性的细粒度优化，为高效训练MoE架构模型提供了关键技术支撑。</p>


                        <h4><i>iii．</i>一种面向多模态模型架构的并行训练优化技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多模态模型融合了文本、图像等多种模态，其架构通常由编码模块、投影模块和主干大规模语言模型三个模块组成。此外，多模态模型训练常采用冻结策略，即部分模块参数固定不变。现有方法忽略了这种混合的模块结构和动态的参数状态，使得多模态模型在智能计算集群的并行训练存在负载不均和资源浪费的问题。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/MMOH.png" alt="面向多模态模型架构的高效并行训练技术MMOH" style="max-width: 40%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图7：面向多模态模型架构的高效并行训练技术MMOH</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种面向多模态模型架构的高效并行训练技术MMOH，其核心在于协同优化的并行训练规划器与一个感知模型动态状态的调度方法。MMOH首先基于设备的实际算力与显存容量对集群进行排序，并将多模态模型的不同组件按其资源需求精准映射到匹配的设备上，实现需求驱动的拓扑适配。在此基础上MMOH引入混合粒度抽象，对参数量较小的编码模块进行合并以减少通信开销，同时对主干模型模块中的注意力与前馈网络模块进行细粒度拆分，从而达成更均衡的计算负载分布。此外，MMOH将模块是否冻结纳入建模，在划分决策中显式考虑冻结模块的计算与通信节省，以最小化整体迭代时间。实验表明，MMoH相比Megatron-LM、Whale和Espresso等代表性方法，最高可实现多模态模型并行训练1.77倍的端到端加速。该研究解决了多模态模型架构的抽象和调度难题，为训练具有复杂结构和动态状态的智能模型提供了重要思路。</p>



                        <h4><i>iv．</i>一种高效的模型并行策略自动生成技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以流水线并行为代表的模型并行训练是主流的大规模智能模型训练方法之一，当前已有不少研究工作对模型并行策略进行优化，但在流水线并行的划分平衡性和启动开销等方面仍面临挑战。一方面，流水线划分的平衡性影响流水线的训练过程，较差的平衡性会使得训练过程中产生很多气泡，从而延长训练时间；另一方面，流水线的启动开销与流水线的深度成正比，阻碍了流水线往更深发展。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/AutoPipe.png" alt="高效流水线并行策略生成技术AutoPipe" style="max-width: 40%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图8：高效流水线并行策略生成技术AutoPipe</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种基于平衡划分和微批次切分的高效流水线并行策略生成技术AutoPipe。AutoPipe使用启发式算法自动求解模型的流水线划分方案，提高了流水线划分的平衡性。具体而言，AutoPipe首先进行模型的子层粒度切分，然后对流水线运行过程中的依赖关系进行详细分析，对流水线并行训练方法进行了建模，最后，对基于子层粒度切分的流水线划分方案，该方法利用建模得到的模型进行优劣判断，并基于启发式规则来调整当前的流水线划分方案，以产生可能更优的划分方案。AutoPipe方法通过对流水线启动阶段的部分微批次进行切分，提前了各个流水级处理第一个微批次的时间，显著减小了流水线的启动开销。实验结果表明，相比于DAPPLE、Piper、Megatron-LM等方法，AutoPipe方法降低了流水线的启动开销，提升了流水线划分方案的平衡性，加速了智能模型的端到端训练。例如，对GPT-2模型训练的实验结果表明，相比于Megatron-LM方法，AutoPipe方法实现了约1.3倍的端到端训练加速。该研究解决了流水线并行中负载均衡与启动开销两大核心难题，为高效并行策略生成提供了兼具性能与实用性的新范式。</p>


                        <h4><i>v．</i>一种并行训练操作调度策略的全局自动搜索技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式并行训练策略已成为训练大规模智能模型的重要技术，其通过将智能模型进行划分，并采用预定义的训练操作调度策略来实现高效训练。然而，当前并行训练操作调度策略的设计与实现需要大量的专业知识，且现有框架往往支持较单一的默认预定义调度，缺乏对新型高效训练操作执行策略的关注和测试能力。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/Koala.png" alt="大规模智能模型并行训练操作调度的的自动化搜索技术Koala" style="max-width: 60%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图9：大规模智能模型并行训练操作调度的的自动化搜索技术Koala</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种大规模智能模型并行训练操作调度的的自动化搜索技术Koala。Koala设计了一种领域特定语言作为并行训练操作的中间表示形式，实现智能模型并行训练的抽象和调度策略的表示。同时Koala改进了遗传算法，以实现并行训练操作调度优化策略的自动搜索与生成，显著提高了并行训练的可编程性和并行效率。通过统一的系统软件接口和结构设计，Koala实现了自动化的端到端并行训练流程。用户可以通过单程序多数据程序解析领域特定语言，来部署大规模智能模型的高效并行训练。Koala还设计了以子模型为基本执行单元的分布式计算图，实现了计算与通信操作的重叠，从而提高了大规模并行训练的计算效率。在GPT2-XL等模型上进行的对比实验表明，相较于Megatron-LM等技术，Koala方法实现了1.10到1.55倍的并行训练加速。该研究将并行训练的设计范式从人工经验驱动转为自动化搜索驱动，为更复杂的并行训练探索开辟了新路径。</p>



                    </details>





                    <details open="" style="margin-bottom: 10px;">
                        <summary><b>c．异构智能计算环境的协同计算方法</b></summary>
                        
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;智能模型的现实部署环境普遍呈现异构加速器共存的特征，不同代际、不同算力、不同存储容量的智能计算设备常被混合使用，以兼顾成本与性能。这种硬件异构性导致传统同构假设下的并行策略失效：若简单将模型均分至所有设备，低算力或存储的设备将成为性能瓶颈；若仅使用高性能设备，则造成资源浪费。此外，智能模型结构本身也呈现出功能异构性，模型的不同部分在计算强度与内存需求上差异显著，模型异构与硬件异构叠加对协同计算方法提出了更高要求。同时，智能计算环境的异构不仅体现在计算单元，还延伸至网络拓扑乃至多集群层面，使得任务映射、通信协调与负载均衡面临跨层级、跨地域的复杂耦合挑战。</p>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;围绕上述问题，项目系统性地开展了面向硬件异构的并行策略优化、异构功能模型在异构环境的计算方法以及异构多集群平台的协同计算架构等研究，形成了一套贯通算力异构、模型异构到跨集群异构的协同计算技术体系。相关研究发表于NSDI、TC、Cluster、EMNLP等CCF推荐的A类、B类国际期刊与会议。以下四项代表性工作从不同角度协同推进，构建了面向异构智能计算环境的高效协同计算技术体系。</p>
                        
                        <h4><i>i．</i>一种面向异构计算设备的智能计算优化技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着智能计算集群硬件的更新迭代，集群中常包含了不同型号的计算设备。为充分利用异构硬件的计算能力，需要研究面向异构计算环境的模型并行计算加速方法。在异构计算环境中进行模型并行计算，面临因设备计算性能和存储不均衡造成的计算效率低、扩展性差和存储利用率低的问题。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/HPH.png" alt="面向大规模智能模型在异构智能计算环境中的优化技术HPH" style="max-width: 40%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图10：面向大规模智能模型在异构智能计算环境中的优化技术HPH</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种面向大规模智能模型在异构智能计算环境中的优化技术HPH。该技术包括了异构计算环境通信拓扑设计方法、异构计算环境模型划分方法以及基于重计算感知的流水线调度算法。其中通信拓扑设计方法主要提升混合并行中梯度同步通信的效率，提高了HPH方法在异构计算环境中的扩展性；HPH的模型划分方法以最小化流水线执行时间为目标，实现了在异构环境中的模型自动划分，提高了端到端计算效率；基于重计算感知的流水线调度算法对流水线并行中的前向、反向计算和激活值重计算操作进行重新调度，充分利用了设备的空闲时间，实现了在不引入额外计算开销的条件下，降低了存储消耗，并均衡了各个设备的存储压力。在Bert、GPT系列等智能模型上的实验结果表明，HPH方法相比于Megatron-LM方法可以提升24%的计算效率；相比于HetPipe方法可以提升41%的计算性能。该研究系统性地解决了智能计算集群异构性带来的计算效率问题，为低成本、高能效的大规模智能模型计算提供了实用方案。</p>
                        
                        <h4><i>ii．</i>一种面向异构网络的协同智能计算优化技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;智能协同计算过程中，参与计算的设备需要周期性的进行通信，以保证协同计算结果与单机计算结果的一致性。智能计算集群的网络链路存在着异构性，这一异构性会带来通信竞争并影响协同计算性能。目前针对该异构性的研究将通信使用的网络链路分为结点内和结点间两类，结点内使用PCIe，结点间使用InfiniBand或者以太网。然而在部分智能计算集群的计算结点中，只有挂载在同一个CPU上的智能计算设备是只通过PCIe进行通信，挂载在不同CPU上的智能计算设备还需要经过CPU之间的QPI链路才能进行通信。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/APH.png" alt="基于网络链路异构感知的任务设备映射技术APH" style="max-width: 40%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图11：基于网络链路异构感知的任务设备映射技术APH</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种基于网络链路异构感知的任务设备映射技术APH，如图 11所示，其核心是通过对智能模型协同计算过程中的各类通信操作进行建模分析，确定了各类通信操作的通信量大小。结合各类通信操作的通信规模，APH可以得到通信操作在智能计算集群中的优先级。基于这一优先级，APH提出了数据并行优先的设备映射策略，通过将数据并行组映射到相邻的设备上，减少协同计算过程中的通信竞争，提高整体计算性能。实验结果表明，相较于随机的设备映射策略和流水线优先的设备映射策略，APH能够实现最大1.12倍的计算性能提高。该研究精准捕捉了智能计算集群的网络拓扑特性，为智能模型协同计算提供了高性价比的优化路径。</p>


                        <h4><i>iii．</i>一种面向模型异构加硬件异构的协同计算技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在异构硬件上部署异构智能模型面临严峻挑战。若整个模型仅加载至高性能设备，往往会遇到存储空间不足的问题，若完全依赖CPU设备则受限于其较低的计算能力，难以满足实际需求。当前智能模型可以使用草稿模型加目标模型的方式加速计算，但是这种异构的模型结构在异构设备间频繁的粗粒度数据交换将引入显著通信延迟，削弱了加速效果。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/Dovetail.png" alt="面向异构智能模型结构的协同投机解码计算技术Dovetail" style="max-width: 50%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图12：面向异构智能模型结构的协同投机解码计算技术Dovetail</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种面向异构智能模型结构的协同投机解码计算技术Dovetail。Dovetail重新设计了异构设备间的功能分工，将小型草稿模型部署在 高性能智能计算设备上，充分利用其计算能力快速生成多个候选结果；同时将大型目标模型完整驻留在CPU内存中，借助其容量优势完成对候选结果的验证。在此基础上，Dovetail将以模型层为单位的数据传输细化至张量级别，仅在必要时传递最小验证单元，大幅降低跨设备通信开销。此外，针对异构硬件的计算特性，该方法还引入动态门控融合机制，并对草稿模型的深度与宽度进行联合优化，在控制延迟的同时提升其预测准确率，从而延长每次验证所接受的结果数量。实验表明，Dovetail在LLaMA-13B等大规模智能模型上实现了1.79倍至10.1倍的端到端协同投机解码计算加速。该研究提出的协同计算方法为在资源受限环境中实现异构智能模型部署提供了切实可行的技术路径。</p>



                        <h4><i>iv．</i>一种面向跨异构计算集群的协同计算技术</h4>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;随着大规模智能模型对计算资源的需求已逐渐超越单个计算集群的承载能力，需联合多个物理上独立的计算集群协同完成。然而，各集群在计算性能、网络带宽以及硬件可靠性方面存在显著异构性，当前的跨集群协同计算方法因依赖全局同步和去中心化的集合通信，面临着跨集群通信开销打、异步计算支持不足以及局部故障的容忍能力差等挑战。</p>
                        <div style="text-align: center; margin: 10px 0;">
                            <img src="tags/Di-PS.png" alt="基于新型去中心化参数服务器架构的智能模型跨异构集群协同计算技术Di-PS" style="max-width: 50%; border: 0px solid #ccc; padding: 5px;">
                            <p style="font-style: italic; color: #666;">图13：基于新型去中心化参数服务器架构的智能模型跨异构集群协同计算技术Di-PS</p>
                        </div>
                        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目研究提出了一种基于新型去中心化参数服务器架构的智能模型跨异构集群协同计算技术Di-PS。Di-PS将模型计算过程解耦为内层与外层两个层次，每个集群内部独立维护一个优化器副本，自主执行高频的内层参数更新，从而减少对远程通信的依赖；集群间的外层同步则通过点对点通信实现，避免了集群间网络异构的影响，提升了系统整体的弹性与吞吐能力。此外，Di-PS实现了跨集群的故障隔离，任一集群发生故障时，其余集群可继续协同计算，显著增强了系统的容错能力。实验表明，相比现有的谷歌DiLoCo方法，Di-PS可将集群间通信效率提升1.39倍，并将整体外层同步过程加速1.21倍。该研究不仅验证了跨集群协同训练在超大规模场景下的可行性，更提供了一种可扩展、高容错、适应跨地域异构基础设施的新范式。</p>



                    </details>
















                </article>




                <div class="gdoc-page__footer flex flex-wrap justify-between">
    
    
                </div>
                
                            </div>
                        </main>
                
                      
                    </div>
                
                    
                
                
                
                
                <script defer src="/js/en.search.min.js"></script>
                
                
                
                <script defer src="/js/clipboard.min.js"></script>
                <script>
                    document.addEventListener("DOMContentLoaded", function(event) {
                        var clipboard = new ClipboardJS('.clip');
                    });
                </script>
                
                
</body>
</html>
